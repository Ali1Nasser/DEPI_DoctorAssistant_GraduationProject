{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tix6bIWvFmQZ"
      },
      "source": [
        "# Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7J9H846typ_",
        "outputId": "ec07d008-28e6-45d6-b684-42c9be4efead"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "gien8KpHFxXi",
        "outputId": "0fd0b405-a4ce-46c2-f26e-a49aa50e431b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.11.0\n",
            "  Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.3.25)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.11.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.11.0)\n",
            "Collecting keras<2.12,>=2.11.0 (from tensorflow==2.11.0)\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.1)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.11.0)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Collecting tensorboard<2.12,>=2.11 (from tensorflow==2.11.0)\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0 (from tensorflow==2.11.0)\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.32.3)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n",
            "Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, tensorflow-estimator, tensorboard-data-server, protobuf, keras, gast, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-aiplatform 1.68.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.26.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigtable 2.26.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-functions 1.16.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-iam 2.15.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-language 2.13.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-pubsub 2.25.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-resource-manager 1.12.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-translate 3.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "googleapis-common-protos 1.65.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "pandas-gbq 0.23.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.6 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.19.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.11.0 protobuf-3.19.6 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "3d3125f82d17427794cfc35e631b813b",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install tensorflow==2.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XroNOjcIFxUQ",
        "outputId": "742a714a-15af-497d-8737-cc8cccaa3f36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "82QvQwxfFxRq",
        "outputId": "c434c727-5b9b-44de-cf2d-1706a7b66dfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: protobuf 3.19.6\n",
            "Uninstalling protobuf-3.19.6:\n",
            "  Successfully uninstalled protobuf-3.19.6\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall protobuf -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "collapsed": true,
        "id": "MvDv8NqRFxP4",
        "outputId": "413dd501-aa7c-4777-d6be-7a3cc8276a45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting protobuf==3.20.*\n",
            "  Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
            "Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "Installing collected packages: protobuf\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.23.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "de500465829e43659c9165db13f0d17f",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install protobuf==3.20.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GeP6efgJFxNI",
        "outputId": "cff1b964-42b6-4e33-d862-928c0f32d73a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.39.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (16.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.8.1)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<6,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.39.0-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: watchdog, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.39.0 watchdog-5.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KhO9d_1FxKw",
        "outputId": "046ba873-eec8-4936-8539-bad6e143969c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install Pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7hyA0tEFxIJ"
      },
      "outputs": [],
      "source": [
        "%env PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T93dlcbtFxEg",
        "outputId": "77521dd9-1dd7-4bad-f521-9085031c89d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2mdKor2sRARzy00rzHF4tijetfp_34n4ANTCXK9egrXRg4m3G\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rAHhdjbK0jsK",
        "outputId": "1ac83abd-20be-4910-b130-a19d53c0ff1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting azure-ai-language-questionanswering\n",
            "  Downloading azure_ai_language_questionanswering-1.1.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting azure-core\n",
            "  Downloading azure_core-1.31.0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting isodate<1.0.0,>=0.6.1 (from azure-ai-language-questionanswering)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from azure-core) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from azure-core) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core) (2024.8.30)\n",
            "Downloading azure_ai_language_questionanswering-1.1.0-py3-none-any.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.1/113.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core-1.31.0-py3-none-any.whl (197 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.4/197.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: isodate, azure-core, azure-ai-language-questionanswering\n",
            "Successfully installed azure-ai-language-questionanswering-1.1.0 azure-core-1.31.0 isodate-0.6.1\n"
          ]
        }
      ],
      "source": [
        "pip install azure-ai-language-questionanswering azure-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "m9GrH_ww1Tap",
        "outputId": "2454e7a6-5886-4584-f1c2-7acb11c3b80b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting azure-cognitiveservices-speech\n",
            "  Downloading azure_cognitiveservices_speech-1.40.0-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Downloading azure_cognitiveservices_speech-1.40.0-py3-none-manylinux1_x86_64.whl (40.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: azure-cognitiveservices-speech\n",
            "Successfully installed azure-cognitiveservices-speech-1.40.0\n",
            "Requirement already satisfied: azure-ai-language-questionanswering in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: azure-core<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from azure-ai-language-questionanswering) (1.31.0)\n",
            "Requirement already satisfied: isodate<1.0.0,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from azure-ai-language-questionanswering) (0.6.1)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-language-questionanswering) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-language-questionanswering) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-language-questionanswering) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-language-questionanswering) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-language-questionanswering) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-language-questionanswering) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-language-questionanswering) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install azure-cognitiveservices-speech\n",
        "!pip install azure-ai-language-questionanswering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "-----\n",
        "-----\n",
        "-----\n",
        "-----\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LqreTxrhk76B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDguEojVi4x-"
      },
      "source": [
        "# With the Chatbot First"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Run the three Cells below**\n",
        "### **And then open the link in the first cell's output that looks like \"https://---.ngrok-free.app\" while running the third cell**"
      ],
      "metadata": {
        "id": "MqXy-HY5l28h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax97xqNEtaJg",
        "outputId": "27a2e9ab-a0da-4c55-aed2-46e5125ad8a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"https://f872-34-148-155-183.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Start an HTTP tunnel on the default Streamlit port (8501)\n",
        "public_url = ngrok.connect(8501, \"http\")\n",
        "print(f\"Public URL: {public_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MjTZWp6i3nH",
        "outputId": "85ca02b3-8fa9-44c6-a365-2ec70921de9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "# Save as app.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import azure.cognitiveservices.speech as speechsdk\n",
        "import os\n",
        "from azure.ai.language.questionanswering import QuestionAnsweringClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# Azure Speech-to-Text API credentials\n",
        "speech_key = \"dbd409c216334bd9914dc82fcc104c7b\"\n",
        "service_region = \"eastasia\"\n",
        "\n",
        "# Azure Q&A API credentials\n",
        "qa_endpoint = \"https://qabraintumor.cognitiveservices.azure.com/\"\n",
        "qa_key = \"ebb5337031664837892317f393dbc60e\"\n",
        "\n",
        "# Load your trained model\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/DEPI_Graduation_project/full_model.h5')\n",
        "\n",
        "# Initialize Q&A client\n",
        "qa_client = QuestionAnsweringClient(qa_endpoint, AzureKeyCredential(qa_key))\n",
        "\n",
        "# Define the background image using HTML\n",
        "background_img_html = '''\n",
        "    <style>\n",
        "    .stApp {\n",
        "        background-image: url(\"https://i.imgur.com/hyzsSeC.jpeg\");\n",
        "        background-size: cover;\n",
        "        background-position: center;\n",
        "        background-repeat: no-repeat;\n",
        "    }\n",
        "    </style>\n",
        "'''\n",
        "\n",
        "# Inject the custom background image style\n",
        "st.markdown(background_img_html, unsafe_allow_html=True)\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"MRI Brain Tumor Classification\")\n",
        "\n",
        "# Upload image\n",
        "uploaded_file = st.file_uploader(\"Choose an MRI image...\", type=\"jpg\")\n",
        "\n",
        "# Custom CSS for prediction and button styling\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "    .predicted-class {\n",
        "        font-size: 28px;\n",
        "        font-weight: bold;\n",
        "        color: white;\n",
        "        text-align: center;\n",
        "        background-color: #0c6087;\n",
        "        padding: 15px;\n",
        "        border-radius: 10px;\n",
        "        box-shadow: 3px 3px 15px rgba(0, 0, 0, 0.3);\n",
        "        margin-top: 20px;\n",
        "    }\n",
        "    .stButton>button {\n",
        "        background-color: #4CAF50;\n",
        "        color: white;\n",
        "        padding: 20px 24px;\n",
        "        border-radius: 8px;\n",
        "        font-size: 18px;\n",
        "        text-align: center;\n",
        "        font-weight: bold;\n",
        "        box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.2);\n",
        "        transition: background-color 0.3s ease;\n",
        "    }\n",
        "    .stButton>button:hover {\n",
        "        background-color: #45a049;\n",
        "    }\n",
        "    </style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Function to use Azure Text-to-Speech to generate speech\n",
        "def speak_prediction_lively(prediction_text):\n",
        "    # Configure Azure Speech SDK\n",
        "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
        "    audio_output = speechsdk.audio.AudioOutputConfig(filename=\"prediction_audio.wav\")  # Save as a .wav file\n",
        "\n",
        "    # Create a synthesizer\n",
        "    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_output)\n",
        "\n",
        "    # Synthesize speech\n",
        "    result = synthesizer.speak_text_async(prediction_text).get()\n",
        "\n",
        "    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
        "        # st.write(\"Speech synthesized successfully.\")\n",
        "        # Play the audio in Streamlit\n",
        "        audio_file = open(\"prediction_audio.wav\", \"rb\")\n",
        "        audio_bytes = audio_file.read()\n",
        "        st.audio(audio_bytes, format=\"audio/wav\")\n",
        "    elif result.reason == speechsdk.ResultReason.Canceled:\n",
        "        cancellation_details = result.cancellation_details\n",
        "        st.error(f\"Speech synthesis canceled: {cancellation_details.reason}\")\n",
        "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
        "            st.error(f\"Error details: {cancellation_details.error_details}\")\n",
        "\n",
        "\n",
        "\n",
        "# If an image is uploaded\n",
        "if uploaded_file is not None:\n",
        "    img = Image.open(uploaded_file)\n",
        "    st.image(img, caption='Uploaded Image.', use_column_width=True)\n",
        "\n",
        "    # Preprocess the image for the model\n",
        "    img = img.resize((299, 299))\n",
        "    img_array = np.array(img)\n",
        "\n",
        "    # If the image is grayscale (2D), convert to 3D (adding channels)\n",
        "    if img_array.ndim == 2:\n",
        "        img_array = np.stack((img_array,) * 3, axis=-1)\n",
        "    elif img_array.shape[-1] == 1:\n",
        "        img_array = np.concatenate((img_array, img_array, img_array), axis=-1)\n",
        "\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(img_array)\n",
        "    st.markdown(f\"<div class='prediction-text'>Raw predictions: {np.array2string(predictions, formatter={'float_kind': lambda x: f'{x:.2f}'})}</div>\", unsafe_allow_html=True)\n",
        "\n",
        "    # Get the predicted class\n",
        "    predicted_class = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Class mapping\n",
        "    class_labels = {\n",
        "        0: \"Glioma\",\n",
        "        1: \"Meningioma\",\n",
        "        2: \"No Tumor\",\n",
        "        3: \"Pituitary\"\n",
        "    }\n",
        "\n",
        "    # Get the predicted class name\n",
        "    prediction_text = class_labels[predicted_class[0]]\n",
        "\n",
        "    # Display the prediction with custom styling\n",
        "    st.markdown(f\"<div class='predicted-class'>Predicted Class: {prediction_text}</div>\", unsafe_allow_html=True)\n",
        "\n",
        "    # Automatically speak the prediction\n",
        "    speak_prediction_lively('You have ' + prediction_text)\n",
        "\n",
        "# Q&A section\n",
        "st.subheader(\"Ask a Question\")\n",
        "question = st.text_input(\"What would you like to know about tumors?\")\n",
        "\n",
        "# Define project and deployment names (from your Azure Language resource)\n",
        "project_name = \"BrainTumorQnA\"\n",
        "deployment_name = \"production\"\n",
        "\n",
        "if question:\n",
        "    response = qa_client.get_answers(\n",
        "        question=question,\n",
        "        top=1,\n",
        "        project_name=project_name,\n",
        "        deployment_name=deployment_name\n",
        "    )\n",
        "\n",
        "    if response.answers:\n",
        "        st.write(\"BrainTumorQA: \", response.answers[0].answer)\n",
        "        speak_prediction_lively(response.answers[0].answer)\n",
        "    else:\n",
        "        st.write(\"Sorry, I couldn't find an answer to that question.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4ycWAI_xi3k6",
        "outputId": "655517d8-5976-47db-8398-7f8a41f93c01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.148.155.183:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2024-10-03 15:33:16.320 Uncaught exception GET /_stcore/stream (127.0.0.1)\n",
            "HTTPServerRequest(protocol='http', host='f872-34-148-155-183.ngrok-free.app', method='GET', uri='/_stcore/stream', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/websocket.py\", line 937, in _accept_connection\n",
            "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/web/server/browser_websocket_handler.py\", line 126, in open\n",
            "    self._session_id = self._runtime.connect_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/runtime.py\", line 384, in connect_session\n",
            "    session_id = self._session_mgr.connect_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/websocket_session_manager.py\", line 99, in connect_session\n",
            "    session = AppSession(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/app_session.py\", line 133, in __init__\n",
            "    self._pages_manager = PagesManager(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/pages_manager.py\", line 236, in __init__\n",
            "    self.pages_strategy = PagesManager.DefaultStrategy(self, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/pages_manager.py\", line 74, in __init__\n",
            "    PagesStrategyV1.watch_pages_dir(pages_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/pages_manager.py\", line 62, in watch_pages_dir\n",
            "    watch_dir(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/path_watcher.py\", line 155, in watch_dir\n",
            "    return _watch_path(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/path_watcher.py\", line 126, in _watch_path\n",
            "    watcher_class(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/event_based_path_watcher.py\", line 107, in __init__\n",
            "    path_watcher.watch_path(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/event_based_path_watcher.py\", line 185, in watch_path\n",
            "    folder_handler.watch = self._observer.schedule(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/api.py\", line 312, in schedule\n",
            "    emitter.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/utils/__init__.py\", line 75, in start\n",
            "    self.on_thread_start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify.py\", line 119, in on_thread_start\n",
            "    self._inotify = InotifyBuffer(path, recursive=self.watch.is_recursive, event_mask=event_mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_buffer.py\", line 30, in __init__\n",
            "    self._inotify = Inotify(path, recursive=recursive, event_mask=event_mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_c.py\", line 167, in __init__\n",
            "    self._add_dir_watch(path, event_mask, recursive=recursive)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_c.py\", line 394, in _add_dir_watch\n",
            "    self._add_watch(full_path, mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_c.py\", line 407, in _add_watch\n",
            "    Inotify._raise_error()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_c.py\", line 424, in _raise_error\n",
            "    raise OSError(err, os.strerror(err))\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "Exception ignored in: <function AppSession.__del__ at 0x7a4f80e11870>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/app_session.py\", line 174, in __del__\n",
            "    self.shutdown()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/app_session.py\", line 240, in shutdown\n",
            "    if self._state != AppSessionState.SHUTDOWN_REQUESTED:\n",
            "AttributeError: 'AppSession' object has no attribute '_state'\n",
            "2024-10-03 15:33:18.472236: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-03 15:33:19.798855: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-10-03 15:33:19.798898: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2024-10-03 15:33:22.388538: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-10-03 15:33:22.388821: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-10-03 15:33:22.388850: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2024-10-03 15:33:28.041451: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-10-03 15:33:28.044067: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2024-10-03 15:33:28.044167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (368d4ebc3940): /proc/driver/nvidia/version does not exist\n",
            "2024-10-03 15:33:28.045242: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py &"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "-----\n",
        "-----\n",
        "-----\n",
        "-----\n"
      ],
      "metadata": {
        "id": "Jl06JT7ak3Xs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuBJV83lVg75"
      },
      "source": [
        "# Integration (Brain Tumor & Ocular Disease)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Run the three Cells below**\n",
        "### **And then open the link in the first cell's output that looks like \"https://---.ngrok-free.app\" while running the third cell**"
      ],
      "metadata": {
        "id": "HjupH_tgnK1H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXycQQseVgEJ",
        "outputId": "64e495aa-c4b1-4ae1-e06d-efa59965d837"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"https://ba2b-34-148-155-183.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Start an HTTP tunnel on the default Streamlit port (8501)\n",
        "public_url = ngrok.connect(8501, \"http\")\n",
        "print(f\"Public URL: {public_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_IqWZMcWAgB",
        "outputId": "1e4334ec-48a1-4e25-85b0-f0351cb24826"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import azure.cognitiveservices.speech as speechsdk\n",
        "import os\n",
        "from azure.ai.language.questionanswering import QuestionAnsweringClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# Azure Speech-to-Text API credentials\n",
        "speech_key = \"dbd409c216334bd9914dc82fcc104c7b\"\n",
        "service_region = \"eastasia\"\n",
        "\n",
        "# Azure Q&A API credentials\n",
        "qa_endpoint = \"https://qabraintumor.cognitiveservices.azure.com/\"\n",
        "qa_key = \"ebb5337031664837892317f393dbc60e\"\n",
        "\n",
        "\n",
        "# Load your models\n",
        "brain_tumor_model = tf.keras.models.load_model('/content/drive/MyDrive/DEPI_Graduation_project/full_model.h5')\n",
        "eye_disease_models = {\n",
        "    \"Myopia\": tf.keras.models.load_model('/content/drive/MyDrive/DEPI_Graduation_project/weights/vgg19_all_datasets_full_df_myopia_aug.h5'),\n",
        "    \"Macular Degeneration\": tf.keras.models.load_model('/content/drive/MyDrive/DEPI_Graduation_project/weights/vgg19_all_datasets_full_df_macular_degeneration_aug.h5'),\n",
        "    \"Hypertensive\": tf.keras.models.load_model('/content/drive/MyDrive/DEPI_Graduation_project/weights/vgg19_all_datasets_full_df_hypertensive_aug.h5'),\n",
        "    \"Glaucoma\": tf.keras.models.load_model('/content/drive/MyDrive/DEPI_Graduation_project/weights/vgg19_all_datasets_full_df_glaucoma_aug.h5'),\n",
        "    \"Diabetic\": tf.keras.models.load_model('/content/drive/MyDrive/DEPI_Graduation_project/weights/vgg19_all_datasets_full_df_diabetic_aug.h5'),\n",
        "    \"Cataract\": tf.keras.models.load_model('/content/drive/MyDrive/DEPI_Graduation_project/weights/vgg19_all_datasets_full_df_cataract_aug.h5'),\n",
        "    \"Other\": tf.keras.models.load_model('/content/drive/MyDrive/DEPI_Graduation_project/weights/vgg19_all_datasets_full_df_other_aug.h5')\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "# Initialize Q&A client\n",
        "qa_client = QuestionAnsweringClient(qa_endpoint, AzureKeyCredential(qa_key))\n",
        "\n",
        "# Custom background image and title\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "    .stApp {\n",
        "        background-image: url(\"https://i.imgur.com/hyzsSeC.jpeg\");\n",
        "        background-size: cover;\n",
        "    }\n",
        "    </style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "st.title(\"Medical Image Classification\")\n",
        "\n",
        "# Selection box to choose between Brain Tumor and Eye Disease\n",
        "disease_type = st.selectbox(\"Choose the classification type\", (\"Brain Tumor\", \"Eye Disease\"))\n",
        "\n",
        "# Upload image\n",
        "uploaded_file = st.file_uploader(\"Choose an image...\", type=\"jpg\")\n",
        "\n",
        "# Function to speak prediction\n",
        "def speak_prediction_lively(prediction_text):\n",
        "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
        "    audio_output = speechsdk.audio.AudioOutputConfig(filename=\"prediction_audio.wav\")\n",
        "    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_output)\n",
        "    result = synthesizer.speak_text_async(prediction_text).get()\n",
        "    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
        "        audio_file = open(\"prediction_audio.wav\", \"rb\")\n",
        "        audio_bytes = audio_file.read()\n",
        "        st.audio(audio_bytes, format=\"audio/wav\")\n",
        "\n",
        "# Classification logic\n",
        "if uploaded_file is not None:\n",
        "    img = Image.open(uploaded_file)\n",
        "    st.image(img, caption='Uploaded Image.', use_column_width=True)\n",
        "\n",
        "     # Preprocess the image\n",
        "    if disease_type == \"Brain Tumor\":\n",
        "        # Resize image to (299, 299) for brain tumor model\n",
        "        img = img.resize((299, 299))\n",
        "    else:\n",
        "        # Resize image to (224, 224) for eye disease models\n",
        "        img = img.resize((224, 224))\n",
        "\n",
        "    img_array = np.array(img)\n",
        "\n",
        "    if img_array.ndim == 2:  # Grayscale to RGB\n",
        "        img_array = np.stack((img_array,) * 3, axis=-1)\n",
        "    elif img_array.shape[-1] == 1:\n",
        "        img_array = np.concatenate((img_array, img_array, img_array), axis=-1)\n",
        "\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0\n",
        "\n",
        "    # Run predictions based on disease type\n",
        "    if disease_type == \"Brain Tumor\":\n",
        "        predictions = brain_tumor_model.predict(img_array)\n",
        "        class_labels = {0: \"Glioma\", 1: \"Meningioma\", 2: \"No Tumor\", 3: \"Pituitary\"}\n",
        "          # Get the predicted class\n",
        "        predicted_class = np.argmax(predictions, axis=1)\n",
        "        prediction_text = class_labels[predicted_class[0]]\n",
        "\n",
        "        # Display the prediction and speak it\n",
        "        st.markdown(f\"<div class='predicted-class'>Predicted Class: {prediction_text}</div>\", unsafe_allow_html=True)\n",
        "        speak_prediction_lively('You have ' + prediction_text)\n",
        "\n",
        "    else:  # Eye Disease selected\n",
        "        # Prediction and confidence calculation\n",
        "        predictions = {name: model.predict(img_array) for name, model in eye_disease_models.items()}\n",
        "        results = {name: 'Yes' if pred > 0.5 else 'No' for name, pred in predictions.items()}\n",
        "        confidences = {name: round(float(pred) * 100, 2) for name, pred in predictions.items()}\n",
        "\n",
        "         # Displaying results in Streamlit\n",
        "        st.subheader(\"Eye Disease Classification Results:\")\n",
        "        Diseases = \"\"\n",
        "        for name, confidence in confidences.items():\n",
        "            result = results[name]\n",
        "            st.write(f\"{name}: {result} (Confidence: {confidence}%)\")\n",
        "            Diseases += f\"{name}: {result} (Confidence: {confidence}%)\\n\"\n",
        "        speak_prediction_lively('You have ' + Diseases)\n",
        "\n",
        "\n",
        "\n",
        "# Q&A section\n",
        "st.subheader(\"Ask a Question\")\n",
        "question = st.text_input(\"What would you like to know?\")\n",
        "\n",
        "if question:\n",
        "    response = qa_client.get_answers(question=question, top=1, project_name=\"BrainTumorQnA\", deployment_name=\"production\")\n",
        "    if response.answers:\n",
        "        st.write(\"Response:\", response.answers[0].answer)\n",
        "        speak_prediction_lively(response.answers[0].answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UOelq0-CWJmA",
        "outputId": "3f9a0a8f-b90e-48f9-bbd4-0b4250b59d0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.148.155.183:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2024-10-03 15:34:27.127 Uncaught exception GET /_stcore/stream (127.0.0.1)\n",
            "HTTPServerRequest(protocol='http', host='ba2b-34-148-155-183.ngrok-free.app', method='GET', uri='/_stcore/stream', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/websocket.py\", line 937, in _accept_connection\n",
            "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/web/server/browser_websocket_handler.py\", line 126, in open\n",
            "    self._session_id = self._runtime.connect_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/runtime.py\", line 384, in connect_session\n",
            "    session_id = self._session_mgr.connect_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/websocket_session_manager.py\", line 99, in connect_session\n",
            "    session = AppSession(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/app_session.py\", line 133, in __init__\n",
            "    self._pages_manager = PagesManager(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/pages_manager.py\", line 236, in __init__\n",
            "    self.pages_strategy = PagesManager.DefaultStrategy(self, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/pages_manager.py\", line 74, in __init__\n",
            "    PagesStrategyV1.watch_pages_dir(pages_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/pages_manager.py\", line 62, in watch_pages_dir\n",
            "    watch_dir(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/path_watcher.py\", line 155, in watch_dir\n",
            "    return _watch_path(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/path_watcher.py\", line 126, in _watch_path\n",
            "    watcher_class(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/event_based_path_watcher.py\", line 107, in __init__\n",
            "    path_watcher.watch_path(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/event_based_path_watcher.py\", line 185, in watch_path\n",
            "    folder_handler.watch = self._observer.schedule(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/api.py\", line 312, in schedule\n",
            "    emitter.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/utils/__init__.py\", line 75, in start\n",
            "    self.on_thread_start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify.py\", line 119, in on_thread_start\n",
            "    self._inotify = InotifyBuffer(path, recursive=self.watch.is_recursive, event_mask=event_mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_buffer.py\", line 30, in __init__\n",
            "    self._inotify = Inotify(path, recursive=recursive, event_mask=event_mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_c.py\", line 167, in __init__\n",
            "    self._add_dir_watch(path, event_mask, recursive=recursive)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_c.py\", line 394, in _add_dir_watch\n",
            "    self._add_watch(full_path, mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_c.py\", line 407, in _add_watch\n",
            "    Inotify._raise_error()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_c.py\", line 424, in _raise_error\n",
            "    raise OSError(err, os.strerror(err))\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "Exception ignored in: <function AppSession.__del__ at 0x7ae4f6659870>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/app_session.py\", line 174, in __del__\n",
            "    self.shutdown()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/app_session.py\", line 240, in shutdown\n",
            "    if self._state != AppSessionState.SHUTDOWN_REQUESTED:\n",
            "AttributeError: 'AppSession' object has no attribute '_state'\n",
            "2024-10-03 15:34:28.637186: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-03 15:34:28.818895: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-10-03 15:34:28.818942: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2024-10-03 15:34:30.132738: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-10-03 15:34:30.132896: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-10-03 15:34:30.132925: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2024-10-03 15:34:32.724687: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-10-03 15:34:32.725598: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2024-10-03 15:34:32.725967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (368d4ebc3940): /proc/driver/nvidia/version does not exist\n",
            "2024-10-03 15:34:32.726915: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 1s 887ms/step\n",
            "1/1 [==============================] - 1s 855ms/step\n",
            "1/1 [==============================] - 1s 846ms/step\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ae4758ebd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 1s 849ms/step\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ae4758e9120> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 1s 824ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "/content/app.py:103: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  confidences = {name: round(float(pred) * 100, 2) for name, pred in predictions.items()}\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 876ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 844ms/step\n",
            "/content/app.py:103: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  confidences = {name: round(float(pred) * 100, 2) for name, pred in predictions.items()}\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "Exception ignored in atexit callback: <function load_source.<locals>.<lambda> at 0x7ae480037760>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/loader.py\", line 57, in <lambda>\n",
            "    atexit.register(lambda: _remove_file(file_name))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/loader.py\", line 34, in _remove_file\n",
            "    os.remove(file_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/web/bootstrap.py\", line 44, in signal_handler\n",
            "    server.stop()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/web/server/server.py\", line 417, in stop\n",
            "    self._runtime.stop()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/runtime.py\", line 324, in stop\n",
            "    async_objs.eventloop.call_soon_threadsafe(stop_on_eventloop)\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 798, in call_soon_threadsafe\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 515, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py &"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "-----\n",
        "-----\n",
        "-----\n",
        "-----\n"
      ],
      "metadata": {
        "id": "-RJk-U9HlJf6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD32nLzAyn_c"
      },
      "source": [
        "# With Different Pages in Streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Run the three Cells below**\n",
        "### **And then open the link in the first cell's output that looks like \"https://---.ngrok-free.app\" while running the third cell**"
      ],
      "metadata": {
        "id": "NlGJ5blznMLU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb9Nsz6pyq6o",
        "outputId": "8e78057c-214f-4690-ba78-b449340443d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"https://c73b-34-148-155-183.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "# Start an HTTP tunnel on the default Streamlit port (8501)\n",
        "public_url = ngrok.connect(8501, \"http\")\n",
        "print(f\"Public URL: {public_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E4AOiCFyrn9",
        "outputId": "d0ea360a-bff5-4da6-cabd-257c0703893c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import azure.cognitiveservices.speech as speechsdk\n",
        "import os\n",
        "from azure.ai.language.questionanswering import QuestionAnsweringClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# Azure Speech-to-Text API credentials\n",
        "speech_key = \"dbd409c216334bd9914dc82fcc104c7b\"\n",
        "service_region = \"eastasia\"\n",
        "\n",
        "# Azure Q&A API credentials\n",
        "qa_endpoint = \"https://qabraintumor.cognitiveservices.azure.com/\"\n",
        "qa_key = \"ebb5337031664837892317f393dbc60e\"\n",
        "\n",
        "# Load your models\n",
        "brain_tumor_model = tf.keras.models.load_model('/content/drive/MyDrive/DEPI_Graduation_project/full_model.h5')\n",
        "eye_disease_models = {\n",
        "    \"Myopia\": tf.keras.models.load_model('/content/drive/MyDrive/DEPI_Graduation_project/weights/vgg19_all_datasets_full_df_myopia_aug.h5'),\n",
        "    \"Macular Degeneration\": tf.keras.models.load_model('/content/drive/MyDrive/DEPI_Graduation_project/weights/vgg19_all_datasets_full_df_macular_degeneration_aug.h5'),\n",
        "    \"Hypertensive\": tf.keras.models.load_model('/content/drive/MyDrive/DEPI_Graduation_project/weights/vgg19_all_datasets_full_df_hypertensive_aug.h5'),\n",
        "    \"Glaucoma\": tf.keras.models.load_model('/content/drive/MyDrive/DEPI_Graduation_project/weights/vgg19_all_datasets_full_df_glaucoma_aug.h5'),\n",
        "    \"Diabetic\": tf.keras.models.load_model('/content/drive/MyDrive/DEPI_Graduation_project/weights/vgg19_all_datasets_full_df_diabetic_aug.h5'),\n",
        "    \"Cataract\": tf.keras.models.load_model('/content/drive/MyDrive/DEPI_Graduation_project/weights/vgg19_all_datasets_full_df_cataract_aug.h5'),\n",
        "    \"Other\": tf.keras.models.load_model('/content/drive/MyDrive/DEPI_Graduation_project/weights/vgg19_all_datasets_full_df_other_aug.h5')\n",
        "}\n",
        "\n",
        "# Initialize Q&A client\n",
        "qa_client = QuestionAnsweringClient(qa_endpoint, AzureKeyCredential(qa_key))\n",
        "\n",
        "# Custom background image and title\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "    .stApp {\n",
        "        background-image: url(\"https://i.imgur.com/hyzsSeC.jpeg\");\n",
        "        background-size: cover;\n",
        "    }\n",
        "    </style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "st.title(\"Medical Image Classification\")\n",
        "\n",
        "# Sidebar for page selection\n",
        "page = st.sidebar.selectbox(\"Choose a page\", [\"Brain Tumor Classification\", \"Eye Disease Classification\"])\n",
        "\n",
        "# Function to speak prediction\n",
        "def speak_prediction_lively(prediction_text):\n",
        "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
        "    audio_output = speechsdk.audio.AudioOutputConfig(filename=\"prediction_audio.wav\")\n",
        "    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_output)\n",
        "    result = synthesizer.speak_text_async(prediction_text).get()\n",
        "    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
        "        audio_file = open(\"prediction_audio.wav\", \"rb\")\n",
        "        audio_bytes = audio_file.read()\n",
        "        st.audio(audio_bytes, format=\"audio/wav\")\n",
        "\n",
        "# Page 1: Brain Tumor Classification\n",
        "if page == \"Brain Tumor Classification\":\n",
        "    st.header(\"Brain Tumor Classification\")\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Upload an MRI image for brain tumor classification...\", type=\"jpg\")\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        img = Image.open(uploaded_file)\n",
        "        st.image(img, caption='Uploaded Image.', use_column_width=True)\n",
        "\n",
        "        # Preprocess the image (Resize to 299, 299 for brain tumor model)\n",
        "        img = img.resize((299, 299))\n",
        "        img_array = np.array(img)\n",
        "        if img_array.ndim == 2:  # Grayscale to RGB\n",
        "            img_array = np.stack((img_array,) * 3, axis=-1)\n",
        "        elif img_array.shape[-1] == 1:\n",
        "            img_array = np.concatenate((img_array, img_array, img_array), axis=-1)\n",
        "\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = img_array / 255.0\n",
        "\n",
        "        # Predict\n",
        "        predictions = brain_tumor_model.predict(img_array)\n",
        "        class_labels = {0: \"Glioma\", 1: \"Meningioma\", 2: \"No Tumor\", 3: \"Pituitary\"}\n",
        "        predicted_class = np.argmax(predictions, axis=1)\n",
        "        prediction_text = class_labels[predicted_class[0]]\n",
        "\n",
        "        # Display the prediction and speak it\n",
        "        st.markdown(f\"<div class='predicted-class'>Predicted Class: {prediction_text}</div>\", unsafe_allow_html=True)\n",
        "        speak_prediction_lively('You have ' + prediction_text)\n",
        "\n",
        "# Page 2: Eye Disease Classification\n",
        "elif page == \"Eye Disease Classification\":\n",
        "    st.header(\"Eye Disease Classification\")\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Upload an eye image for disease classification...\", type=\"jpg\")\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        img = Image.open(uploaded_file)\n",
        "        st.image(img, caption='Uploaded Image.', use_column_width=True)\n",
        "\n",
        "        # Preprocess the image (Resize to 224, 224 for eye disease models)\n",
        "        img = img.resize((224, 224))\n",
        "        img_array = np.array(img)\n",
        "        if img_array.ndim == 2:  # Grayscale to RGB\n",
        "            img_array = np.stack((img_array,) * 3, axis=-1)\n",
        "        elif img_array.shape[-1] == 1:\n",
        "            img_array = np.concatenate((img_array, img_array, img_array), axis=-1)\n",
        "\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = img_array / 255.0\n",
        "\n",
        "        # Predict and display results\n",
        "        predictions = {name: model.predict(img_array) for name, model in eye_disease_models.items()}\n",
        "        results = {name: 'Yes' if pred > 0.5 else 'No' for name, pred in predictions.items()}\n",
        "        confidences = {name: round(float(pred) * 100, 2) for name, pred in predictions.items()}\n",
        "\n",
        "        st.subheader(\"Eye Disease Classification Results:\")\n",
        "        diseases = \"\"\n",
        "        for name, confidence in confidences.items():\n",
        "            result = results[name]\n",
        "            st.write(f\"{name}: {result} (Confidence: {confidence}%)\")\n",
        "            diseases += f\"{name}: {result} (Confidence: {confidence}%)\\n\"\n",
        "\n",
        "        speak_prediction_lively('You have ' + diseases)\n",
        "\n",
        "# Q&A section\n",
        "st.subheader(\"Ask a Question\")\n",
        "question = st.text_input(\"What would you like to know?\")\n",
        "\n",
        "if question:\n",
        "    response = qa_client.get_answers(question=question, top=1, project_name=\"BrainTumorQnA\", deployment_name=\"production\")\n",
        "    if response.answers:\n",
        "        st.write(\"Response:\", response.answers[0].answer)\n",
        "        speak_prediction_lively(response.answers[0].answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzxx6NDhyvvd",
        "outputId": "ca71ef06-37e4-43d8-88c5-d36b3c912cef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.148.155.183:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2024-10-03 16:29:02.749 Uncaught exception GET /_stcore/stream (127.0.0.1)\n",
            "HTTPServerRequest(protocol='http', host='c73b-34-148-155-183.ngrok-free.app', method='GET', uri='/_stcore/stream', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/websocket.py\", line 937, in _accept_connection\n",
            "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/web/server/browser_websocket_handler.py\", line 126, in open\n",
            "    self._session_id = self._runtime.connect_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/runtime.py\", line 384, in connect_session\n",
            "    session_id = self._session_mgr.connect_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/websocket_session_manager.py\", line 99, in connect_session\n",
            "    session = AppSession(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/app_session.py\", line 133, in __init__\n",
            "    self._pages_manager = PagesManager(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/pages_manager.py\", line 236, in __init__\n",
            "    self.pages_strategy = PagesManager.DefaultStrategy(self, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/pages_manager.py\", line 74, in __init__\n",
            "    PagesStrategyV1.watch_pages_dir(pages_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/pages_manager.py\", line 62, in watch_pages_dir\n",
            "    watch_dir(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/path_watcher.py\", line 155, in watch_dir\n",
            "    return _watch_path(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/path_watcher.py\", line 126, in _watch_path\n",
            "    watcher_class(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/event_based_path_watcher.py\", line 107, in __init__\n",
            "    path_watcher.watch_path(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/event_based_path_watcher.py\", line 185, in watch_path\n",
            "    folder_handler.watch = self._observer.schedule(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/api.py\", line 312, in schedule\n",
            "    emitter.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/utils/__init__.py\", line 75, in start\n",
            "    self.on_thread_start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify.py\", line 119, in on_thread_start\n",
            "    self._inotify = InotifyBuffer(path, recursive=self.watch.is_recursive, event_mask=event_mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_buffer.py\", line 30, in __init__\n",
            "    self._inotify = Inotify(path, recursive=recursive, event_mask=event_mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_c.py\", line 167, in __init__\n",
            "    self._add_dir_watch(path, event_mask, recursive=recursive)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_c.py\", line 394, in _add_dir_watch\n",
            "    self._add_watch(full_path, mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_c.py\", line 407, in _add_watch\n",
            "    Inotify._raise_error()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_c.py\", line 424, in _raise_error\n",
            "    raise OSError(err, os.strerror(err))\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "Exception ignored in: <function AppSession.__del__ at 0x7f0ead24d870>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/app_session.py\", line 174, in __del__\n",
            "    self.shutdown()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/app_session.py\", line 240, in shutdown\n",
            "    if self._state != AppSessionState.SHUTDOWN_REQUESTED:\n",
            "AttributeError: 'AppSession' object has no attribute '_state'\n",
            "2024-10-03 16:29:04.530973: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-03 16:29:06.202700: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-10-03 16:29:06.202749: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2024-10-03 16:29:09.962466: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-10-03 16:29:09.963595: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-10-03 16:29:09.963630: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2024-10-03 16:29:16.059888: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-10-03 16:29:16.064898: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2024-10-03 16:29:16.065002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (368d4ebc3940): /proc/driver/nvidia/version does not exist\n",
            "2024-10-03 16:29:16.065959: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 853ms/step\n",
            "1/1 [==============================] - 1s 864ms/step\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0e1343e3b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 1s 858ms/step\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0e1343c310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 1s 852ms/step\n",
            "1/1 [==============================] - 1s 853ms/step\n",
            "1/1 [==============================] - 1s 853ms/step\n",
            "/content/app.py:113: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  confidences = {name: round(float(pred) * 100, 2) for name, pred in predictions.items()}\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py &"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The End**"
      ],
      "metadata": {
        "id": "zbmWEr_HlVX7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}